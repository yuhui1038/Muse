#!/usr/bin/env python3
"""
Evaluate Mulan-T score for SPLIT audio chunks (sliced from full audio).

This script bridges the gap between:
1. The metadata generated by `prepare_chunks.py` (which might FILTER OUT some chunks).
2. The split audio files generated by `split_audio_by_tokens.py` (which includes ALL chunks to maintain sync).

It matches them by re-scanning the original JSONL to find the 'global index' of each chunk,
so we can pick the correct file from the `chunks/` folder.
"""

import argparse
import json
import os
import sys
import re
from tqdm import tqdm
import torch
import numpy as np
import librosa

try:
    from muq import MuQMuLan
except ImportError:
    # Add project root to path if needed
    # sys.path.append("xxx/Muse/")
    # from muq import MuQMuLan
    raise ImportError("Please install MuQ or add it to Python path")

def extract_audio_codes(text: str):
    """Extract <AUDIO_XXXX> -> [int, ...]"""
    return [int(x) for x in re.findall(r"<AUDIO_(\d+)>", text)]

def extract_dsec(text: str):
    # Support both dsec and desc
    match = re.search(r"\[(?:dsec|desc):(.*?)]", text, re.DOTALL)
    return match.group(1).strip() if match else None

def get_chunk_mapping(original_jsonl_path):
    """
    Simulates the logic of BOTH scripts to build a map:
    metadata_index -> (song_idx, sub_chunk_idx)
    
    Returns:
        list of dicts: [{'song_idx': 0, 'sub_idx': 2}, ...] 
        The i-th element corresponds to the i-th line in the FLAT jsonl/metadata file.
    """
    mapping = []
    
    with open(original_jsonl_path, 'r', encoding='utf-8') as f:
        for song_idx, line in enumerate(f):
            if not line.strip(): continue
            try:
                data = json.loads(line)
            except:
                continue
            
            messages = data.get("messages", [])
            
            # Logic to track "Global Sub Index" (what split_audio_by_tokens.py uses)
            global_sub_idx = 0
            
            # State for prepare_chunks logic
            current_prompt = None
            
            for msg in messages:
                role = msg.get("role")
                content = msg.get("content", "")
                
                if role == "user":
                    dsec = extract_dsec(content)
                    if dsec:
                        current_prompt = dsec
                
                elif role == "assistant":
                    # Check if this turn actually has audio
                    # (split_audio_by_tokens extracts tokens directly)
                    tokens = extract_audio_codes(content)
                    if not tokens:
                        continue
                        
                    # This is a valid audio chunk in the Full Audio.
                    # So split_audio_by_tokens.py will save it as {song_idx}_{global_sub_idx}.wav
                    
                    # Now checks if prepare_chunks.py would include it
                    if current_prompt:
                        # Yes, this chunk is included in the evaluation set.
                        mapping.append({
                            "song_idx": song_idx,
                            "sub_idx": global_sub_idx,
                            "prompt": current_prompt
                        })
                        
                        # Reset prompt as it's consumed
                        current_prompt = None
                    
                    # Increment the global counter because this chunk exists in the audio file
                    global_sub_idx += 1

    return mapping

def main():
    parser = argparse.ArgumentParser()
    # meta_xxx.jsonl file generated by prepare_chunks.py
    parser.add_argument("--metadata", required=True, help="Path to meta_xxx.jsonl")
    
    # Directory containing the original JSONLs (needed to rebuild mapping)
    # usually .../testdata/output_xxx
    parser.add_argument("--original_jsonl_dir", required=True, help="Directory containing original task .jsonl files")
    
    # Directory containing the split chunks (the 'chunks' folder inside the task dir)
    # But wait, tasks have subdirs. 
    # Let's ask for the ROOT of the task directory (where split_audio_by_tokens.py ran)
    # e.g. .../output_5e-4_1.3_main
    parser.add_argument("--task_root_dir", required=True, help="Root directory of the task (containing subdirs with wavs)")
    
    parser.add_argument("--output", required=True, help="Path to save output results")
    parser.add_argument("--model", default="MuQ-MuLan-large", help="MuQ-MuLan model name or path")
    parser.add_argument("--gpu", type=int, default=0)
    args = parser.parse_args()

    device = f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu"
    print(f"[INFO] Using device: {device}")
    
    print(f"[INFO] Loading Mulan model from {args.model}...")
    model = MuQMuLan.from_pretrained(args.model).to(device).eval()

    # 1. Load Metadata
    print(f"[INFO] Loading metadata from {args.metadata}...")
    meta_items = []
    with open(args.metadata, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                meta_items.append(json.loads(line))
    
    # 2. Rebuild Mapping
    # The metadata items contain "original_jsonl": "filename.jsonl"
    # We can group them by file and process.
    
    # Group items by original jsonl file to avoid re-parsing the file many times
    items_by_file = {}
    for i, item in enumerate(meta_items):
        fname = item["original_jsonl"]
        if fname not in items_by_file:
            items_by_file[fname] = []
        items_by_file[fname].append((i, item)) # Store index to put back in order later

    # Results list
    results = [None] * len(meta_items)
    all_scores = []
    
    print("[INFO] Processing files and evaluating...")
    
    for jsonl_name, grouped_items in items_by_file.items():
        # Path to original jsonl
        original_path = os.path.join(args.original_jsonl_dir, jsonl_name)
        if not os.path.exists(original_path):
            print(f"[WARN] Original jsonl not found: {original_path}. Skipping these items.")
            continue
            
        # Get the mapping for this file
        # Returns list of {song_idx, sub_idx, prompt} corresponding to the SEQUENTIAL chunks that SHOULD exist in metadata
        # Wait, the mapping logic in 'get_chunk_mapping' simulates the linear scan of prepare_chunks.
        # So the K-th item in 'mapping' should correspond to the K-th item in 'grouped_items' (sorted by flat_index).
        
        # Sort grouped items by their flat_index to ensure alignment
        grouped_items.sort(key=lambda x: x[1]["flat_index"])
        
        # Generate the ground truth mapping from file structure
        file_mapping = get_chunk_mapping(original_path)
        
        if len(file_mapping) != len(grouped_items):
            print(f"[ERROR] Mismatch in chunk count for {jsonl_name}!")
            print(f"  Metadata expects: {len(grouped_items)}")
            print(f"  Re-scan found:    {len(file_mapping)}")
            print("  This suggests logic mismatch. Skipping file to avoid bad data.")
            continue
            
        # Subdirectory for chunks
        subdir_name = os.path.splitext(jsonl_name)[0]
        chunks_dir = os.path.join(args.task_root_dir, subdir_name, "chunks")
        
        for (original_idx, meta_item), map_item in zip(grouped_items, file_mapping):
            # Double check prompt alignment
            if meta_item["text_prompt"] != map_item["prompt"]:
                # This is a strong sanity check
                # Note: exact string match might fail on whitespace, but usually reliable.
                pass 
            
            song_idx = map_item["song_idx"]
            sub_idx = map_item["sub_idx"]
            
            # Construct filename: {song_idx:06d}_{sub_idx:04d}.wav
            wav_name = f"{song_idx:06d}_{sub_idx:04d}.wav"
            wav_path = os.path.join(chunks_dir, wav_name)
            
            if not os.path.exists(wav_path):
                print(f"[WARN] Audio chunk missing: {wav_path}")
                continue
                
            # Evaluate
            try:
                text = meta_item["text_prompt"]
                
                # Check if we need to load audio first
                # From usage in other files: model(wavs=wav_tensor) or model(texts=[text])
                
                wav, _ = librosa.load(wav_path, sr=24000) # MuLan often uses 24k
                wav_tensor = torch.tensor(wav).unsqueeze(0).to(device)
                
                with torch.no_grad():
                    # Calculate embeddings separately
                    audio_emb = model(wavs=wav_tensor)
                    text_emb = model(texts=[text])
                    # Calculate similarity
                    sim = model.calc_similarity(audio_emb, text_emb).item()
                
                score = sim
                
                all_scores.append(score)
                
                result_entry = {
                    "original_jsonl": jsonl_name,
                    "song_idx": song_idx,
                    "sub_idx": sub_idx,
                    "text_prompt": text,
                    "audio_path": wav_path,
                    "score": score
                }
                results[original_idx] = result_entry
                
            except Exception as e:
                print(f"[ERROR] Failed to eval {wav_path}: {e}")

    # Save results
    final_results = [r for r in results if r is not None]
    
    with open(args.output, "w", encoding="utf-8") as f:
        for r in final_results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
            
    if all_scores:
        avg_score = sum(all_scores) / len(all_scores)
        print(f"\n[SUMMARY] Evaluated {len(all_scores)} chunks.")
        print(f"[SUMMARY] Average Mulan-T Score: {avg_score:.4f}")
    else:
        print("[WARN] No scores computed.")

if __name__ == "__main__":
    main()

